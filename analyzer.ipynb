{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import pandas as pd\n",
    "\n",
    "# df = pd.read_csv('datasets/users.csv')\n",
    "\n",
    "# duplicate_user_ids = df['user_id'].value_counts()\n",
    "# duplicate_user_ids = duplicate_user_ids[duplicate_user_ids > 1]\n",
    "\n",
    "# print(f\"Total number of duplicate ids: {len(duplicate_user_ids)}\")\n",
    "\n",
    "# # Json\n",
    "# duplicate_user_ids_dict = duplicate_user_ids.to_dict()\n",
    "# with open('duplicate_user_ids.json', 'w') as json_file:\n",
    "#     json.dump(duplicate_user_ids_dict, json_file, indent=4)\n",
    "\n",
    "# print(\"Results saved in 'duplicate_user_ids.json'\")\n",
    "\n",
    "# # CSV\n",
    "# duplicate_user_ids_df = duplicate_user_ids.reset_index()\n",
    "# duplicate_user_ids_df.columns = ['user_id', 'count']\n",
    "# duplicate_user_ids_df.to_csv('duplicate_user_ids.csv', index=False)\n",
    "\n",
    "# print(\"Results saved in 'duplicate_user_ids.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# df = pd.read_csv('datasets/users_with_duplicates.csv')\n",
    "# df_unique = df.drop_duplicates(subset='user_id', keep='first')\n",
    "# df_unique.to_csv('datasets/users.csv', index=False)\n",
    "\n",
    "# print(\"Duplicates removed and saved to 'cleaned_users.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The column 'status' consists of 2 unique types.\n",
      "The unique types are:\n",
      "['NEW' 'OLD']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import csv\n",
    "\n",
    "csv_file_path = 'datasets/reviews.csv'\n",
    "# csv_file_path = 'datasets/details.csv'\n",
    "column = 'status'\n",
    "\n",
    "df = pd.read_csv(csv_file_path)\n",
    "# print(df.isna().sum())\n",
    "unique_types_count = df[column].nunique()\n",
    "unique_types = df[column].unique()\n",
    "print(f\"The column '{column}' consists of {unique_types_count} unique types.\")\n",
    "print(\"The unique types are:\")\n",
    "print(unique_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of entries in the 'search.json' dataset: 2400\n",
      "Total number of entries in the 'details.csv' dataset: 2400\n",
      "Total number of entries in the 'reviews.csv' dataset: 6822\n",
      "Total number of entries in the 'users.csv' dataset: 5289\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import csv\n",
    "\n",
    "search_path = 'datasets/search.json'\n",
    "# json_file_path = 'locations/lga-scc-pairs.json'\n",
    "details_path = 'datasets/details.csv'\n",
    "reviews_path = 'datasets/reviews.csv'\n",
    "users_path = 'datasets/users.csv'\n",
    "\n",
    "with open(search_path, 'r', encoding='utf-8') as file:\n",
    "    search = json.load(file)\n",
    "\n",
    "details = pd.read_csv(details_path, on_bad_lines='warn')\n",
    "reviews = pd.read_csv(reviews_path, on_bad_lines='warn')\n",
    "users = pd.read_csv(users_path, on_bad_lines='warn')\n",
    "\n",
    "search_len = len(search)\n",
    "details_len = len(details)\n",
    "reviews_len = len(reviews)\n",
    "users_len = len(users)\n",
    "print(f\"Total number of entries in the 'search.json' dataset: {search_len}\")\n",
    "print(f\"Total number of entries in the 'details.csv' dataset: {details_len}\")\n",
    "print(f\"Total number of entries in the 'reviews.csv' dataset: {reviews_len}\")\n",
    "print(f\"Total number of entries in the 'users.csv' dataset: {users_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of entries in the dataset: 2400\n",
      "No duplicate location_id found.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def find_duplicate_ids_in_json(json_file_path, feature):\n",
    "    try:\n",
    "        with open(json_file_path, 'r', encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"The file {json_file_path} does not exist.\")\n",
    "        return\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"The file {json_file_path} is not a valid JSON file or is empty.\")\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    num_rows = len(df)\n",
    "    print(f\"Total number of entries in the dataset: {num_rows}\")\n",
    "\n",
    "    if feature not in df.columns:\n",
    "        print(f\"The column {feature} does not exist in the dataset.\")\n",
    "        return\n",
    "\n",
    "    duplicate_ids = df[df.duplicated(subset=[feature], keep=False)]\n",
    "    \n",
    "    if duplicate_ids.empty:\n",
    "        print(f\"No duplicate {feature} found.\")\n",
    "    else:\n",
    "        num_duplicates = len(duplicate_ids)\n",
    "        print(f\"Found {num_duplicates} duplicate {feature} entries:\\n\")\n",
    "        print(duplicate_ids)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # feature = 'scc'\n",
    "    # json_file_path = 'locations/lga-scc-pairs.json'\n",
    "    feature = 'location_id'\n",
    "    json_file_path = 'datasets/search.json'\n",
    "    find_duplicate_ids_in_json(json_file_path, feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of entries in the dataset: 5289\n",
      "No duplicate user_id found.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def find_duplicate_ids(csv_file_path):\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"The file {csv_file_path} does not exist.\")\n",
    "        return\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(f\"The file {csv_file_path} is empty.\")\n",
    "        return\n",
    "\n",
    "    num_rows = len(df)\n",
    "    print(f\"Total number of entries in the dataset: {num_rows}\")\n",
    "\n",
    "    if column not in df.columns:\n",
    "        print(f\"The column {column} does not exist in the dataset.\")\n",
    "        return\n",
    "    \n",
    "    duplicate_ids = df[df.duplicated(subset=[column], keep=False)]\n",
    "    \n",
    "    if duplicate_ids.empty:\n",
    "        print(f\"No duplicate {column} found.\")\n",
    "    else:\n",
    "        num_duplicates = len(duplicate_ids)\n",
    "        print(f\"Found {num_duplicates} duplicate {column} entries:\\n\")\n",
    "        print(duplicate_ids)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    column = 'user_id'\n",
    "    dataset = 'users'\n",
    "    csv_file_path = f'datasets/{dataset}.csv'\n",
    "    find_duplicate_ids(csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"location_id\": \"50cb504ee4b0c490b6b7c432\",\n",
      "    \"category\": \"restaurant\",\n",
      "    \"name\": \"Spicy Village Restaurant\",\n",
      "    \"address_obj\": {\n",
      "        \"city\": \"\\u0635\\u0644\\u0627\\u0644\\u0629\",\n",
      "        \"state\": \"\\u0638\\u0641\\u0627\\u0631\",\n",
      "        \"country\": \"\\u0639\\u0645\\u0627\\u0646\",\n",
      "        \"address\": \"Lulu Hypermarket\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def print_nth_element(data, n):\n",
    "    if not data:\n",
    "        print(\"Data is empty or not loaded properly\")\n",
    "        return\n",
    "    \n",
    "    if n < 0 or n >= len(data):\n",
    "        print(\"Index out of range\")\n",
    "        return\n",
    "    \n",
    "    nth_element = data[n]\n",
    "    print(json.dumps(nth_element, indent=4))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    json_file_path = 'datasets/search.json'\n",
    "    # json_file_path = 'locations/lga-scc-pairs.json'\n",
    "    try:\n",
    "        with open(json_file_path, 'r', encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"File not found: {e}\")\n",
    "        data = []\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"JSON decode error: {e}\")\n",
    "        data = []\n",
    "\n",
    "    print_nth_element(data, 1828)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
