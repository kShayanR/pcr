{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shaya\\AppData\\Local\\Temp\\ipykernel_19320\\2007926.py:3: DtypeWarning: Columns (7,9,10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  reviews = pd.read_csv('datasets/reviews.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30863 = 33044 \n",
      "---\n",
      "title\n",
      "sentiment\n",
      "visit_type\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "reviews = pd.read_csv('datasets/reviews.csv')\n",
    "users = pd.read_csv('datasets/users.csv')\n",
    "\n",
    "start = 0\n",
    "stop = 68953\n",
    "subset = reviews.iloc[start:stop]\n",
    "print(f\"{subset['user_id'].nunique()} = {len(users)} \\n---\")\n",
    "# print(users.isna().sum())\n",
    "\n",
    "for i in [7, 9, 10]:\n",
    "    col = reviews.columns[i]\n",
    "    print(col)\n",
    "    # print(reviews[col].dtype)\n",
    "    # print(reviews[col].isna().sum())\n",
    "    # print(reviews[col].apply(type).value_counts())\n",
    "    # print(reviews[col][~reviews[col].apply(lambda x: isinstance(x, (int, float)))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shaya\\AppData\\Local\\Temp\\ipykernel_19320\\4080338036.py:16: DtypeWarning: Columns (7,9,10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  reviews = pd.read_csv(reviews_path, on_bad_lines='warn')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of entries in the 'search.json' dataset: 3650\n",
      "Total number of entries in the 'details.csv' dataset: 3650\n",
      "Total number of entries in the 'reviews.csv' dataset: 71260\n",
      "Total number of entries in the 'users.csv' dataset: 33044\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "location = 'datasets'\n",
    "\n",
    "search_path = f'{location}/search.json'\n",
    "# json_file_path = 'locations/lga-scc-pairs.json'\n",
    "details_path = f'{location}/details.csv'\n",
    "reviews_path = f'{location}/reviews.csv'\n",
    "users_path = f'{location}/users.csv'\n",
    "\n",
    "with open(search_path, 'r', encoding='utf-8') as file:\n",
    "    search = json.load(file)\n",
    "\n",
    "details = pd.read_csv(details_path, on_bad_lines='warn')\n",
    "reviews = pd.read_csv(reviews_path, on_bad_lines='warn')\n",
    "users = pd.read_csv(users_path, on_bad_lines='warn')\n",
    "\n",
    "search_len = len(search)\n",
    "details_len = len(details)\n",
    "reviews_len = len(reviews)\n",
    "users_len = len(users)\n",
    "print(f\"Total number of entries in the 'search.json' dataset: {search_len}\")\n",
    "print(f\"Total number of entries in the 'details.csv' dataset: {details_len}\")\n",
    "print(f\"Total number of entries in the 'reviews.csv' dataset: {reviews_len}\")\n",
    "print(f\"Total number of entries in the 'users.csv' dataset: {users_len}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of Foursquare locations: 3079\n",
      "Total number of Foursquare locations with no reviews: 552\n",
      "Total number of reviews for Foursquare locations within range: 69561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shaya\\AppData\\Local\\Temp\\ipykernel_17012\\3695766338.py:8: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  locations_with_no_reviews = foursquare_locations[details_df['num_reviews'] == 0]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def sum_foursquare_reviews(details_file, start_idx, stop_idx):\n",
    "    details_df = pd.read_csv(details_file)\n",
    "    subset_data = details_df.iloc[start_idx:stop_idx]\n",
    "    foursquare_locations = subset_data[details_df['source'] == 'FourSquare']\n",
    "    total_reviews = foursquare_locations['num_reviews'].sum()\n",
    "    locations_with_no_reviews = foursquare_locations[details_df['num_reviews'] == 0]\n",
    "    total_locations = len(foursquare_locations)\n",
    "    total_locations_with_no_reviews = len(locations_with_no_reviews)\n",
    "    return total_reviews, total_locations, total_locations_with_no_reviews\n",
    "\n",
    "\n",
    "start = 0\n",
    "stop = 3650\n",
    "details_file = 'datasets/details.csv'\n",
    "total_reviews, total_locations, total_nlocations = sum_foursquare_reviews(details_file, start, stop)\n",
    "print(f'Total number of Foursquare locations: {total_locations}')\n",
    "print(f'Total number of Foursquare locations with no reviews: {total_nlocations}')\n",
    "print(f'Total number of reviews for Foursquare locations within range: {total_reviews}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicate Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('datasets/users.csv')\n",
    "\n",
    "duplicate_user_ids = df['user_id'].value_counts()\n",
    "duplicate_user_ids = duplicate_user_ids[duplicate_user_ids > 1]\n",
    "\n",
    "print(f\"Total number of duplicate ids: {len(duplicate_user_ids)}\")\n",
    "\n",
    "# Json\n",
    "duplicate_user_ids_dict = duplicate_user_ids.to_dict()\n",
    "with open('duplicate_user_ids.json', 'w') as json_file:\n",
    "    json.dump(duplicate_user_ids_dict, json_file, indent=4)\n",
    "\n",
    "print(\"Results saved in 'duplicate_user_ids.json'\")\n",
    "\n",
    "# CSV\n",
    "duplicate_user_ids_df = duplicate_user_ids.reset_index()\n",
    "duplicate_user_ids_df.columns = ['user_id', 'count']\n",
    "duplicate_user_ids_df.to_csv('duplicate_user_ids.csv', index=False)\n",
    "\n",
    "print(\"Results saved in 'duplicate_user_ids.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicate Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('datasets/users_with_duplicates.csv')\n",
    "df_unique = df.drop_duplicates(subset='user_id', keep='first')\n",
    "df_unique.to_csv('datasets/users.csv', index=False)\n",
    "\n",
    "print(\"Duplicates removed and saved to 'cleaned_users.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unique Values Classifier CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shaya\\AppData\\Local\\Temp\\ipykernel_17012\\3870956539.py:9: DtypeWarning: Columns (7,9,10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The column 'title' consists of 2095 unique types.\n",
      "The unique values are:\n",
      "[nan 'Great, tasty, affordable comfort food' 'Best in Duqm!!!' ...\n",
      " 'A Memorable Dining Experience with Great Shisha and Kebabs!'\n",
      " 'Beautiful restaurant üôåüèª' 'Good atmosphere']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import csv\n",
    "\n",
    "csv_file_path = 'datasets/reviews.csv'\n",
    "# csv_file_path = 'datasets/details.csv'\n",
    "column = 'title'\n",
    "\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# print(df[column].isna().sum())\n",
    "# df['agree_count'] = df['agree_count'].astype(int)\n",
    "\n",
    "unique_types_count = df[column].nunique()\n",
    "unique_types = df[column].unique()\n",
    "print(f\"The column '{column}' consists of {unique_types_count} unique types.\")\n",
    "print(\"The unique values are:\")\n",
    "print(unique_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unique Values Classifier Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 'category' column consists of 57 unique values.\n",
      "The unique values in the 'category' column are:\n",
      "restaurant,cafe, coffee, and tea house,burger joint,fast food restaurant,turkish restaurant,rooftop bar,bar,lounge,night club,food court,hotel,middle eastern restaurant,italian restaurant,american restaurant,spa,seafood restaurant,cocktail bar,brasserie,plaza,french restaurant,hotel bar,steakhouse,chinese restaurant,asian restaurant,mediterranean restaurant,beach bar,lebanese restaurant,coffee shop,breakfast spot,hookah bar,syrian restaurant,egyptian restaurant,greek restaurant,japanese restaurant,sandwich spot,bakery,theme restaurant,vr cafe,other great outdoors,arcade,food truck,gaming cafe,iraqi restaurant,bistro,dessert shop,tea room,australian restaurant,ice cream parlor,mexican restaurant,pastry shop,moroccan restaurant,english restaurant,gluten-free restaurant,german restaurant,kebab restaurant,dumpling restaurant,caf√©\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "feature = 'category'\n",
    "\n",
    "json_file_path = 'datasets/search.json'\n",
    "with open(json_file_path, 'r', encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "# print(df.isna().sum())\n",
    "unique_lga_count = df[feature].nunique()\n",
    "unique_lgas = df[feature].unique()\n",
    "print(f\"The '{feature}' column consists of {unique_lga_count} unique values.\")\n",
    "print(f\"The unique values in the '{feature}' column are:\")\n",
    "unique_lgas_str = ','.join(unique_lgas)\n",
    "print(unique_lgas_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# categories_to_remove = ['Park', 'Water Park', 'Shopping Plaza', 'Neighborhood']\n",
    "# with open('datasets/search.json', 'r', encoding='utf-8') as f:\n",
    "#     data = json.load(f)\n",
    "\n",
    "#############--- To remove the mentioned categories ---#############\n",
    "# filtered_data = [location for location in data if location['category'] not in categories_to_remove]\n",
    "\n",
    "##########--- To convert the list members to lowercase ---##########\n",
    "# for location in data:\n",
    "#     location['category'] = location['category'].lower()\n",
    "####################################################################\n",
    "\n",
    "# with open('search_filtered.json', 'w', encoding='utf-8') as f:\n",
    "#     json.dump(filtered_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# print(f\"Filtered data has been saved to 'search_filtered.json'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "336\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "search_js = 'datasets/search.json'\n",
    "# lga_scc_js = 'locations/lga-scc-pairs.json'\n",
    "with open(search_js, 'r', encoding='utf-8') as file:\n",
    "    search = json.load(file)\n",
    "\n",
    "print(len(search))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicate Identifier Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of entries in the dataset: 3650\n",
      "No duplicate location_id found.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def find_duplicate_ids_in_json(json_file_path, feature):\n",
    "    try:\n",
    "        with open(json_file_path, 'r', encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"The file {json_file_path} does not exist.\")\n",
    "        return\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"The file {json_file_path} is not a valid JSON file or is empty.\")\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    num_rows = len(df)\n",
    "    print(f\"Total number of entries in the dataset: {num_rows}\")\n",
    "\n",
    "    if feature not in df.columns:\n",
    "        print(f\"The column {feature} does not exist in the dataset.\")\n",
    "        return\n",
    "\n",
    "    duplicate_ids = df[df.duplicated(subset=[feature], keep=\"first\")]\n",
    "    \n",
    "    if duplicate_ids.empty:\n",
    "        print(f\"No duplicate {feature} found.\")\n",
    "    else:\n",
    "        num_duplicates = len(duplicate_ids)\n",
    "        print(f\"Found {num_duplicates} duplicate {feature} entries:\\n\")\n",
    "        print(duplicate_ids)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # feature = 'scc'\n",
    "    # json_file_path = 'locations/lga-scc-pairs.json'\n",
    "    feature = 'location_id'\n",
    "    json_file_path = 'datasets/search.json'\n",
    "    find_duplicate_ids_in_json(json_file_path, feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicate Identifier CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of entries in the dataset: 1558\n",
      "No duplicate user_id found.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def find_duplicate_ids(csv_file_path):\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"The file {csv_file_path} does not exist.\")\n",
    "        return\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(f\"The file {csv_file_path} is empty.\")\n",
    "        return\n",
    "\n",
    "    num_rows = len(df)\n",
    "    print(f\"Total number of entries in the dataset: {num_rows}\")\n",
    "\n",
    "    if column not in df.columns:\n",
    "        print(f\"The column {column} does not exist in the dataset.\")\n",
    "        return\n",
    "    \n",
    "    duplicate_ids = df[df.duplicated(subset=[column], keep=\"first\")]\n",
    "    \n",
    "    if duplicate_ids.empty:\n",
    "        print(f\"No duplicate {column} found.\")\n",
    "    else:\n",
    "        num_duplicates = len(duplicate_ids)\n",
    "        print(f\"Found {num_duplicates} duplicate {column} entries:\\n\")\n",
    "        print(duplicate_ids)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    column = 'user_id'\n",
    "    dataset = 'users'\n",
    "    csv_file_path = f'datasets/{dataset}.csv'\n",
    "    find_duplicate_ids(csv_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"location_id\": \"25234055\",\n",
      "    \"category\": \"restaurant\",\n",
      "    \"name\": \"Blad Al Salaam\",\n",
      "    \"address_obj\": {\n",
      "        \"city\": \"Haima\",\n",
      "        \"state\": \"\",\n",
      "        \"country\": \"Oman\",\n",
      "        \"address\": \"37 Haima, Haima 711 Oman\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def print_nth_element(data, n):\n",
    "    if not data:\n",
    "        print(\"Data is empty or not loaded properly\")\n",
    "        return\n",
    "    \n",
    "    if n < 0 or n >= len(data):\n",
    "        print(\"Index out of range\")\n",
    "        return\n",
    "    \n",
    "    nth_element = data[n]\n",
    "    print(json.dumps(nth_element, indent=4))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    json_file_path = 'datasets/search.json'\n",
    "    # json_file_path = 'locations/lga-scc-pairs.json'\n",
    "    try:\n",
    "        with open(json_file_path, 'r', encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"File not found: {e}\")\n",
    "        data = []\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"JSON decode error: {e}\")\n",
    "        data = []\n",
    "\n",
    "    print_nth_element(data, 2399)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
